{"cells":[{"cell_type":"markdown","id":"313bd388","metadata":{"id":"313bd388"},"source":["# Q-Learning para Labirintos Individuais\n","\n","Este notebook implementa um algoritmo Q-Learning otimizado para resolver labirintos individuais. O código treina um agente separado para cada labirinto e então testa seu desempenho, coletando métricas para comparação.\n","\n","A diferença principal em relação ao algoritmo multi-labirintos é que aqui cada agente é especializado em um único labirinto, o que geralmente resulta em melhor desempenho, mas não generaliza bem para labirintos novos."]},{"cell_type":"markdown","id":"b6e35265","metadata":{"id":"b6e35265"},"source":["## Importações e Configuração"]},{"cell_type":"code","execution_count":null,"id":"071a47ce","metadata":{"id":"071a47ce"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import time\n","import os\n","import csv\n","import tracemalloc\n","from collections import deque\n","import glob\n","import pickle\n","\n","# Define as ações: Cima, Baixo, Esquerda, Direita\n","actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n","\n","# Hiperparâmetros do Q-Learning\n","learning_rate = 0.3\n","discount_factor = 0.9\n","epsilon = 0.1  # Taxa de exploração final\n","num_episodes_per_maze = 5000  # Episódios por labirinto"]},{"cell_type":"markdown","id":"67785845","metadata":{"id":"67785845"},"source":["## Classe para Representação do Labirinto"]},{"cell_type":"code","execution_count":null,"id":"7491daf2","metadata":{"id":"7491daf2"},"outputs":[],"source":["class Maze:\n","    def __init__(self, maze_array):\n","        self.maze = maze_array\n","        self.maze_height, self.maze_width = maze_array.shape\n","        self.start_position = (0, 1)\n","        self.goal_position = (self.maze_width - 1, self.maze_height - 2)\n","\n","    def show_maze(self, path=None):\n","        \"\"\"Visualização simples do labirinto\"\"\"\n","        plt.figure(figsize=(5, 5))\n","        plt.imshow(self.maze, cmap='gray')\n","        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=10)\n","        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=10)\n","\n","        if path:\n","            for position in path:\n","                plt.text(position[0], position[1], \"#\", ha='center', va='center', color='blue', fontsize=8)\n","\n","        plt.xticks([]), plt.yticks([])\n","        plt.show()"]},{"cell_type":"markdown","id":"b7f0d827","metadata":{"id":"b7f0d827"},"source":["## Agente Q-Learning para Labirinto Único\n","\n","Esta implementação do agente Q-Learning é otimizada para treinar em um único labirinto, mantendo uma Q-table específica para o tamanho desse labirinto."]},{"cell_type":"code","execution_count":null,"id":"3522ce72","metadata":{"id":"3522ce72"},"outputs":[],"source":["class QLearningAgent:\n","    def __init__(self, state_shape, num_actions=4, learning_rate=0.1, discount_factor=0.9,\n","                 exploration_start=1.0, exploration_end=0.01, num_episodes=100):\n","        # Inicializa a Q-table para o tamanho específico do labirinto\n","        self.q_table = np.zeros((state_shape[0], state_shape[1], num_actions))\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","        self.exploration_start = exploration_start\n","        self.exploration_end = exploration_end\n","        self.num_episodes = num_episodes\n","        self.num_actions = num_actions\n","\n","        # Métricas de treinamento\n","        self.training_time_ms = 0\n","        self.training_episodes = 0\n","        self.total_updates = 0\n","\n","    def get_exploration_rate(self, current_episode):\n","        \"\"\"Calcula a taxa de exploração decrescente ao longo do tempo\"\"\"\n","        exploration_rate = self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / self.num_episodes)\n","        return exploration_rate\n","\n","    def get_action(self, state, maze, current_episode, train=True):\n","        \"\"\"Seleciona uma ação usando política epsilon-greedy durante treinamento ou greedy durante teste\"\"\"\n","        if train:\n","            exploration_rate = self.get_exploration_rate(current_episode)\n","            if np.random.rand() < exploration_rate:\n","                return np.random.randint(self.num_actions)\n","            else:\n","                return np.argmax(self.q_table[state])\n","        else:  # Durante o teste, sempre use a melhor ação conhecida\n","            return np.argmax(self.q_table[state])\n","\n","    def update_q_table(self, state, action, next_state, reward):\n","        \"\"\"Atualiza a Q-table usando a equação de Bellman\"\"\"\n","        best_next_action = np.argmax(self.q_table[next_state])\n","        current_q_value = self.q_table[state][action]\n","        new_q_value = current_q_value + self.learning_rate * (\n","            reward + self.discount_factor * self.q_table[next_state][best_next_action] - current_q_value\n","        )\n","        self.q_table[state][action] = new_q_value"]},{"cell_type":"markdown","id":"2055a617","metadata":{"id":"2055a617"},"source":["## Funções para Execução de Episódios e Treinamento"]},{"cell_type":"code","execution_count":null,"id":"060a1b77","metadata":{"id":"060a1b77"},"outputs":[],"source":["def finish_episode(agent, maze, current_episode, train=True, goal_reward=100, wall_penalty=-10, step_penalty=-1):\n","    \"\"\"Executa um episódio completo e retorna métricas\"\"\"\n","    current_state = maze.start_position\n","    is_done = False\n","    episode_reward = 0\n","    episode_step = 0\n","    path = [current_state]\n","    nos_visitados = set()\n","    nos_visitados.add(current_state)\n","\n","    # Inicia medição de memória e tempo\n","    tracemalloc.start()\n","    tempo_inicio = time.perf_counter()\n","\n","    while not is_done:\n","        action = agent.get_action(current_state, maze, current_episode, train=train)\n","        next_state_coords = actions[action]\n","        next_state = (current_state[0] + next_state_coords[0], current_state[1] + next_state_coords[1])\n","\n","        # Verifica se está fora dos limites ou bateu na parede\n","        if (next_state[0] < 0 or next_state[0] >= maze.maze_width or\n","            next_state[1] < 0 or next_state[1] >= maze.maze_height or\n","            maze.maze[next_state[1]][next_state[0]] == 0):  # 0 é parede nos .npy\n","            reward = wall_penalty\n","            next_state = current_state  # Permanece no estado atual\n","        # Verifica se chegou ao objetivo\n","        elif next_state == maze.goal_position:\n","            path.append(next_state)\n","            nos_visitados.add(next_state)\n","            reward = goal_reward\n","            is_done = True\n","        # Deu mais um passo, mas não chegou ao objetivo\n","        else:\n","            path.append(next_state)\n","            nos_visitados.add(next_state)\n","            reward = step_penalty\n","\n","        episode_reward += reward\n","        episode_step += 1\n","\n","        if train:\n","            agent.update_q_table(current_state, action, next_state, reward)\n","\n","        current_state = next_state\n","\n","        # Limite de passos para evitar loops infinitos\n","        if episode_step > 10000:\n","            is_done = True\n","            if path[-1] != maze.goal_position:\n","                path = []  # Zera o caminho se não chegou ao objetivo\n","\n","    # Finaliza medição de memória e tempo\n","    tempo_fim = time.perf_counter()\n","    memoria_usada = tracemalloc.get_traced_memory()[1]\n","    tracemalloc.stop()\n","\n","    return {\n","        \"caminho\": path,\n","        \"nos_visitados\": list(nos_visitados),\n","        \"tempo_ms\": (tempo_fim - tempo_inicio) * 1000,\n","        \"memoria_bytes\": memoria_usada,\n","        \"comprimento\": len(path) if path and path[-1] == maze.goal_position else 0,\n","        \"qtd_nos_visitados\": len(nos_visitados),\n","        \"encontrou_caminho\": path and path[-1] == maze.goal_position\n","    }\n","\n","def treinar_agente_em_labirinto_unico(agente, maze, num_episodes):\n","    \"\"\"Treina o agente em um único labirinto por um número de episódios\"\"\"\n","    print(f\"Iniciando treinamento em um único labirinto ({maze.maze_height}x{maze.maze_width})...\")\n","    tempo_inicio_treino = time.perf_counter()\n","\n","    agente.training_episodes = num_episodes\n","    total_updates = 0\n","\n","    for episodio in range(num_episodes):\n","        if (episodio + 1) % 100 == 0:\n","            print(f\"  Episódio de Treinamento {episodio + 1}/{num_episodes}\")\n","        resultado = finish_episode(agente, maze, episodio, train=True)\n","        total_updates += len(resultado[\"caminho\"])\n","\n","    tempo_fim_treino = time.perf_counter()\n","    agente.training_time_ms = (tempo_fim_treino - tempo_inicio_treino) * 1000\n","    agente.total_updates = total_updates\n","\n","    print(f\"Treinamento concluído em {agente.training_time_ms/1000:.2f} segundos!\")\n","    print(f\"Total de episódios: {agente.training_episodes}\")\n","    print(f\"Total de atualizações da Q-table: {agente.total_updates}\")"]},{"cell_type":"markdown","id":"9c8945fc","metadata":{"id":"9c8945fc"},"source":["## Carregamento e Visualização de Labirintos"]},{"cell_type":"code","execution_count":null,"id":"bc17fefd","metadata":{"id":"bc17fefd"},"outputs":[],"source":["def carregar_labirintos(diretorio):\n","    \"\"\"Carrega todos os labirintos de um diretório\"\"\"\n","    labirintos = []\n","    arquivos = sorted(glob.glob(os.path.join(diretorio, \"*.npy\")))\n","\n","    for arquivo in arquivos:\n","        labirinto_array = np.load(arquivo)\n","        labirintos.append(Maze(labirinto_array))\n","\n","    return labirintos\n","\n","def get_dynamic_figsize(width_data, height_data, min_inches=8, max_inches=20):\n","    \"\"\"Calcula o tamanho dinâmico da figura com base nas dimensões dos dados\"\"\"\n","    dpi = plt.rcParams.get('figure.dpi', 100.0)\n","\n","    ideal_w_inches = width_data / dpi\n","    ideal_h_inches = height_data / dpi\n","\n","    final_w_inches = max(min_inches, min(max_inches, ideal_w_inches))\n","    final_h_inches = max(min_inches, min(max_inches, ideal_h_inches))\n","\n","    # Ajustes para manter a proporção adequada\n","    if (ideal_w_inches > max_inches or ideal_h_inches > max_inches) and not (ideal_w_inches > max_inches and ideal_h_inches > max_inches):\n","        if ideal_w_inches > ideal_h_inches:\n","            ratio = height_data / width_data\n","            final_h_inches = final_w_inches * ratio\n","        elif ideal_h_inches > ideal_w_inches:\n","            ratio = width_data / height_data\n","            final_w_inches = final_h_inches * ratio\n","\n","        final_w_inches = max(min_inches, final_w_inches)\n","        final_h_inches = max(min_inches, final_h_inches)\n","\n","    elif ideal_w_inches < min_inches and ideal_h_inches < min_inches and ideal_w_inches != ideal_h_inches:\n","        if ideal_w_inches > ideal_h_inches:\n","            ratio = ideal_h_inches / ideal_w_inches\n","            final_h_inches = final_w_inches * ratio\n","        else:\n","            ratio = ideal_w_inches / ideal_h_inches\n","            final_w_inches = final_h_inches * ratio\n","\n","    return (final_w_inches, final_h_inches)\n","\n","def visualizar_caminho_qlearning(maze, caminho, nos_visitados, title_suffix=\"\", save_path=None):\n","    \"\"\"Visualiza o labirinto com o caminho encontrado e nós visitados. Opcionalmente salva a figura.\"\"\"\n","    visualizacao = np.array(maze.maze.copy(), dtype=int)\n","\n","    # Marcar nós visitados com valor 3\n","    for x, y in nos_visitados:\n","        if visualizacao[y][x] == 1:  # Só marca se for um espaço vazio\n","            visualizacao[y][x] = 3\n","\n","    # Marcar caminho encontrado com valor 2\n","    if caminho and caminho[-1] == maze.goal_position:\n","        for x, y in caminho:\n","            visualizacao[y][x] = 2\n","\n","    # Tamanho dinâmico da figura\n","    figsize = get_dynamic_figsize(maze.maze_width, maze.maze_height)\n","    plt.figure(figsize=figsize)\n","\n","    # Mostrar o labirinto com os caminhos\n","    plt.imshow(visualizacao, cmap='viridis', vmin=0, vmax=3, interpolation='nearest')\n","    plt.axis('off')\n","    plt.title(f\"Caminho e Nós Visitados (Q-Learning) {title_suffix}\")\n","\n","    # Marcar início e fim com símbolos mais visíveis\n","    plt.text(maze.start_position[0], maze.start_position[1], \"S\",\n","             ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n","    plt.text(maze.goal_position[0], maze.goal_position[1], \"G\",\n","             ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n","\n","    # Adicionar legenda\n","    legenda = [\n","        mpatches.Patch(color=plt.colormaps['viridis'](0/3), label='Parede (0)'),\n","        mpatches.Patch(color=plt.colormaps['viridis'](1/3), label='Espaço livre (1)'),\n","        mpatches.Patch(color=plt.colormaps['viridis'](2/3), label='Caminho final (2)'),\n","        mpatches.Patch(color=plt.colormaps['viridis'](3/3), label='Nós visitados (3)')\n","    ]\n","    plt.legend(handles=legenda, loc='upper left', bbox_to_anchor=(1.01, 1.0))\n","    plt.tight_layout(rect=[0, 0, 0.85, 1])\n","\n","    # Salvar a figura se um caminho for fornecido\n","    if save_path:\n","        diretorio = os.path.dirname(save_path)\n","        if diretorio:\n","            os.makedirs(diretorio, exist_ok=True)\n","        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n","        print(f\"Figura salva em: {save_path}\")\n","\n","    plt.show()\n","\n","    # Informações adicionais sobre o resultado\n","    encontrou_caminho = caminho and caminho[-1] == maze.goal_position\n","    print(f\"Caminho encontrado: {encontrou_caminho}\")\n","    if encontrou_caminho:\n","        print(f\"Comprimento do caminho: {len(caminho)}\")\n","    print(f\"Número de nós visitados: {len(nos_visitados)}\")"]},{"cell_type":"markdown","id":"820d66bc","metadata":{"id":"820d66bc"},"source":["## Função Principal: Treinar e Avaliar em Labirintos Individuais\n","\n","Esta função treina um agente específico para cada labirinto e coleta métricas de desempenho."]},{"cell_type":"code","execution_count":null,"id":"7e1b9f3f","metadata":{"id":"7e1b9f3f"},"outputs":[],"source":["def comparar_qlearning_em_labirintos_individuais(diretorio_labirintos, num_episodes_per_maze,\n","                                        salvar_qtables=True, salvar_visualizacoes=True):\n","    \"\"\"Treina e avalia um agente específico para cada labirinto\"\"\"\n","    labirintos = carregar_labirintos(diretorio_labirintos)\n","    all_results = []\n","\n","    # Configuração dos diretórios de saída\n","    csv_output_dir = \"resultados/q_learning\"\n","    os.makedirs(csv_output_dir, exist_ok=True)\n","\n","    csv_output_name = f\"{csv_output_dir}/q_learning_individual_maze_baseline.csv\"\n","\n","    # Diretório para salvar Q-tables\n","    q_table_dir = \"q_tables\"\n","    if salvar_qtables:\n","        os.makedirs(q_table_dir, exist_ok=True)\n","\n","    # Diretório para salvar visualizações\n","    visualizacoes_dir = f\"{csv_output_dir}/visualizacoes\"\n","    if salvar_visualizacoes:\n","        os.makedirs(visualizacoes_dir, exist_ok=True)\n","\n","    with open(csv_output_name, \"w\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\n","            \"algoritmo\", \"tamanho_labirinto\", \"indice_labirinto\",\n","            \"tempo_execucao_ms\", \"nos_visitados\", \"encontrou_caminho\",\n","            \"comprimento_caminho\", \"memoria_kb\",\n","            \"tempo_treinamento_ms\", \"episodios_treinamento\",\n","            \"taxa_aprendizado\", \"fator_desconto\",\n","            \"exploracao_inicial\", \"exploracao_final\",\n","            \"total_atualizacoes\"\n","        ])\n","\n","        for idx, maze in enumerate(labirintos):\n","            maze_name = f\"Labirinto {idx+1}/{len(labirintos)}\"\n","            print(f\"\\n--- Processando {maze_name} ({maze.maze_height}x{maze.maze_width}) ---\")\n","\n","            # 1. Criar um agente específico para este labirinto\n","            agente_individual = QLearningAgent(\n","                state_shape=(maze.maze_height, maze.maze_width),\n","                learning_rate=learning_rate,\n","                discount_factor=discount_factor,\n","                exploration_start=1.0,\n","                exploration_end=epsilon,\n","                num_episodes=num_episodes_per_maze\n","            )\n","\n","            # 2. Treinar o agente neste labirinto\n","            treinar_agente_em_labirinto_unico(agente_individual, maze, num_episodes=num_episodes_per_maze)\n","\n","            # 3. Testar o agente no mesmo labirinto\n","            print(f\"Testando agente em {maze_name} (labirinto de treinamento)...\")\n","            resultado_teste = finish_episode(agente_individual, maze, 0, train=False)\n","\n","            # 4. Caminho para salvar a visualização, se necessário\n","            save_path = None\n","            if salvar_visualizacoes:\n","                save_path = f\"{visualizacoes_dir}/maze_{idx+1}_qlearning.png\"\n","\n","            # 5. Visualizar resultados\n","            visualizar_caminho_qlearning(maze, resultado_teste[\"caminho\"],\n","                                         resultado_teste[\"nos_visitados\"],\n","                                         title_suffix=f\" no {maze_name}\",\n","                                         save_path=save_path)\n","\n","            # 6. Coletar e salvar os resultados no CSV\n","            row_data = [\n","                \"Q-Learning_Baseline\",\n","                f\"{maze.maze_height}x{maze.maze_width}\",\n","                idx + 1,\n","                round(resultado_teste[\"tempo_ms\"], 2),\n","                resultado_teste[\"qtd_nos_visitados\"],\n","                resultado_teste[\"encontrou_caminho\"],\n","                resultado_teste[\"comprimento\"],\n","                round(resultado_teste[\"memoria_bytes\"] / 1024, 2),  # em KB\n","                round(agente_individual.training_time_ms, 2),\n","                agente_individual.training_episodes,\n","                agente_individual.learning_rate,\n","                agente_individual.discount_factor,\n","                agente_individual.exploration_start,\n","                agente_individual.exploration_end,\n","                agente_individual.total_updates\n","            ]\n","            writer.writerow(row_data)\n","            all_results.append(row_data)\n","\n","            # 7. Salvar a Q-table para uso futuro\n","            if salvar_qtables:\n","                q_table_filename = f\"{q_table_dir}/q_table_maze_{idx+1}_alpha{agente_individual.learning_rate}_gamma{agente_individual.discount_factor}_eps{agente_individual.exploration_end}_ep{agente_individual.training_episodes}.pkl\"\n","\n","                with open(q_table_filename, 'wb') as qtable_file:\n","                    pickle.dump(agente_individual.q_table, qtable_file)\n","                print(f\"Q-table salva em: {q_table_filename}\")\n","\n","    print(f\"\\n✅ Todos os resultados salvos em: {csv_output_name}\")\n","    return all_results"]},{"cell_type":"markdown","id":"7643f9f6","metadata":{"id":"7643f9f6"},"source":["## Execução do Experimento\n","\n","Execute esta célula para treinar e testar o algoritmo Q-Learning em cada labirinto individualmente."]},{"cell_type":"code","execution_count":null,"id":"2bdae5ff","metadata":{"id":"2bdae5ff"},"outputs":[],"source":["# Configurações do experimento\n","diretorio_labirintos = \"labirintos/pequenos\"  # Diretório dos labirintos\n","salvar_qtables = False  # Define se as Q-tables serão salvas\n","salvar_visualizacoes = False  # Define se as visualizações serão salvas\n","\n","# Para executar com menos episódios durante testes, descomente e ajuste a linha abaixo\n","# num_episodes_per_maze = 500  # Valor menor para testes rápidos\n","\n","print(f\"Iniciando experimento com labirintos do diretório: {diretorio_labirintos}\")\n","print(f\"Número de episódios por labirinto: {num_episodes_per_maze}\")\n","print(f\"Taxa de aprendizado (alpha): {learning_rate}\")\n","print(f\"Fator de desconto (gamma): {discount_factor}\")\n","print(f\"Taxa de exploração final (epsilon): {epsilon}\")\n","print(f\"Salvar Q-tables: {salvar_qtables}\")\n","print(f\"Salvar visualizações: {salvar_visualizacoes}\")\n","\n","# Execute a função para treinar e testar em cada labirinto individualmente\n","individual_baseline_results = comparar_qlearning_em_labirintos_individuais(\n","    diretorio_labirintos,\n","    num_episodes_per_maze,\n","    salvar_qtables=salvar_qtables,\n","    salvar_visualizacoes=salvar_visualizacoes\n",")\n","\n","print(\"\\nProcesso concluído.\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}